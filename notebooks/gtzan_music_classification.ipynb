{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FbbknIYZGJsB"
      },
      "outputs": [],
      "source": [
        "# Used for debugging\n",
        "import pdb; \n",
        "pdb.set_trace() # define a breakpoint"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Command Description**\n",
        "list Show the current location in the file\n",
        "\n",
        "1.   **h**(elp) Show a list of commands, or find help on a specific command\n",
        "2.   **q**(uit) Quit the debugger and the program\n",
        "3.   **c**(ontinue) Quit the debugger, continue in the program\n",
        "4.   **n**(ext) Go to the next step of the program\n",
        "5.   **Repeat** the previous command\n",
        "6.   **p**(rint) Print variables\n",
        "7.   **s**(tep) Step into a subroutine\n",
        "8.   **r**(eturn) Return out of a subroutine"
      ],
      "metadata": {
        "id": "V8YinfPrQBCt"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MCJKmgpJiVj2"
      },
      "source": [
        "# UTILS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2Tv1SvJfabs8"
      },
      "outputs": [],
      "source": [
        "import librosa\n",
        "import librosa.display\n",
        "\n",
        "def plot_melspectrogram(songname:str):\n",
        "  y, sr = librosa.load(songname, mono=True, duration=2, offset=2*2)\n",
        "  ps = librosa.feature.melspectrogram(y=y, sr=sr, hop_length = 256, n_fft = 512, n_mels=64)\n",
        "  ps = librosa.power_to_db(ps**2)\n",
        "\n",
        "  D = librosa.stft(y)  # STFT of y\n",
        "  S_db = librosa.amplitude_to_db(np.abs(D), ref=np.max)\n",
        "\n",
        "  plt.figure()\n",
        "  librosa.display.specshow(S_db)\n",
        "  plt.colorbar()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2SCUBIF-P10K"
      },
      "outputs": [],
      "source": [
        "# Utilities to run evaluation of classifiers\n",
        "\n",
        "\n",
        "from sklearn.model_selection import KFold, train_test_split \n",
        "from sklearn import metrics\n",
        "\n",
        "# We can perform both KFold validation and hold-out validation, but because of computational cost only the latter is used.\n",
        "def evaluate_classifier(classifier, n_folds, X_train_split, y_train_split, X_validation_split = None, y_validation_split = None, fit = True):\n",
        "  \"\"\"\n",
        "  Utilities to run evaluation of classifiers\n",
        "  The splits are not performed in these methods because validation sets need to be stored in order to have a reliable evaluation of \n",
        "  models that are stored and then reloaded.\n",
        "\n",
        "  If n_folds is falsy, perform a validation with a single test/validation split, using the other parameters\n",
        "\n",
        "  \"\"\"\n",
        "\n",
        "  if n_folds:\n",
        "    accuracy, precision, recall, fmacro, fmicro = _evaluate_classifier_with_folds(classifier, fit, n_folds)\n",
        "  else:\n",
        "    accuracy, precision, recall, fmacro, fmicro = _evaluate_classifier_with_split(classifier, fit, X_train_split, X_validation_split, y_train_split, y_validation_split)\n",
        "   \n",
        "  print(\"Accuracy: \", accuracy)\n",
        "  print(\"Precision: \", precision)\n",
        "  print(\"Recall: \", recall)\n",
        "  print(\"F1-micro: \", fmicro)\n",
        "  print(\"F1-macro: \", fmacro)\n",
        "\n",
        "  return accuracy, precision, recall, fmacro, fmicro\n",
        "\n",
        "\n",
        "def _evaluate_classifier_with_folds(classifier, fit, n_folds):\n",
        "\n",
        "  accuracy, precision, recall, fmacro, fmicro = 0, 0, 0, 0, 0\n",
        "  X_train_array = np.array(X_train)\n",
        "\n",
        "  k_fold = KFold(n_splits=n_folds, random_state=0, shuffle=True)\n",
        "  for idx, (train_index, validation_index) in enumerate(k_fold.split(X_train_array)):\n",
        "    \n",
        "    print(f\"Fold #{idx}\")\n",
        "    print(f\"Fitting the model...\")\n",
        "\n",
        "    if fit:\n",
        "      classifier.fit(X_train_array[train_index], y_train[train_index])\n",
        "    \n",
        "    print(f\"Model fitted. Predicting on validation set...\")\n",
        "    predicted = classifier.predict(X_train_array[validation_index])\n",
        "    accuracy = accuracy + metrics.accuracy_score(y_train[validation_index], predicted)\n",
        "    precision = precision + metrics.precision_score(y_train[validation_index], predicted, average='micro')\n",
        "    recall = recall + metrics.recall_score(y_train[validation_index], predicted, average='micro')\n",
        "    fmicro = fmicro + metrics.f1_score(y_train[validation_index], predicted, average='micro')\n",
        "    fmacro = fmacro + metrics.f1_score(y_train[validation_index], predicted, average='macro')\n",
        "  \n",
        "  accuracy /= n_folds\n",
        "  precision /= n_folds\n",
        "  recall /= n_folds\n",
        "  fmicro /= n_folds\n",
        "  fmacro /= n_folds\n",
        "  \n",
        "  return accuracy, precision, recall, fmacro, fmicro\n",
        "\n",
        "\n",
        "def _evaluate_classifier_with_split(classifier, fit, X_train_split, X_validation_split, y_train_split, y_validation_split):\n",
        "  \n",
        "  if fit:\n",
        "    print(f\"Fitting the model...\")\n",
        "    classifier.fit(X_train_split, y_train_split)\n",
        "  \n",
        "  print(f\"Model fitted. Prediting on validation set...\")\n",
        "  predicted = classifier.predict(X_validation_split)\n",
        "\n",
        "  accuracy = metrics.accuracy_score(y_validation_split, predicted)\n",
        "  precision = metrics.precision_score(y_validation_split, predicted, average='macro')\n",
        "  recall = metrics.recall_score(y_validation_split, predicted, average='macro')  \n",
        "  fmicro = metrics.f1_score(y_validation_split, predicted, average='micro')\n",
        "  fmacro = metrics.f1_score(y_validation_split, predicted, average='macro')\n",
        "\n",
        "  return accuracy, precision, recall, fmacro, fmicro\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "def plot_history(history):\n",
        "    \n",
        "    fig,axs = plt.subplots(2)\n",
        "    axs[0].plot(history.history[\"accuracy\"],label=\"train accuracy\")\n",
        "    axs[0].plot(history.history[\"val_accuracy\"],label=\"test accuracy\")\n",
        "    axs[0].set_ylabel(\"Accuracy\")\n",
        "    axs[0].legend(loc='lower right')\n",
        "    axs[0].set_title(\"Accuracy eval\")\n",
        "    \n",
        "    axs[1].plot(history.history[\"loss\"],label=\"train error\")\n",
        "    axs[1].plot(history.history[\"val_loss\"],label=\"test error\")\n",
        "    axs[1].set_ylabel(\"Error\")\n",
        "    axs[1].set_xlabel(\"Epoch\")\n",
        "    axs[1].legend(loc='upper right')\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "_S4sfcemTdPN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# SONG INFO CLASSIFICATION "
      ],
      "metadata": {
        "id": "-z5BEILCPXAo"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DCf-K7NeC1is"
      },
      "source": [
        "## Dataset *Preprocessing*"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "import pandas as pd\n",
        "\n",
        "dataset_url = \"features_3_sec.csv\"\n",
        "dataset = pd.read_csv(dataset_url, sep = ',')\n",
        "dataset_X = dataset.drop(['label', 'filename'], axis=1)\n",
        "dataset_Y =  np.array(dataset.label.astype(int))\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(dataset_X, dataset_Y, test_size=0.33, random_state=0)\n",
        "X_train_split, X_validation, y_train_split, y_validation = train_test_split(X_train, y_train, test_size=0.33, random_state=0)"
      ],
      "metadata": {
        "id": "vqANBqHU38a8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"X_train: {X_train.shape}\")\n",
        "print(f\"y_train: {y_train.shape}\")\n",
        "print(\"\\n\")\n",
        "print(f\"X_test: {X_test.shape}\")\n",
        "print(f\"y_test: {y_test.shape}\")\n",
        "print(\"\\n\")\n",
        "print(f\"X_train_split: {X_train_split.shape}\")\n",
        "print(f\"y_train_split: {y_train_split.shape}\")\n",
        "print(\"\\n\")\n",
        "print(f\"X_validation: {X_validation.shape}\")\n",
        "print(f\"y_validation: {y_validation.shape}\")"
      ],
      "metadata": {
        "id": "3nwRYSKeJjF0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O36YEZc-aygP"
      },
      "outputs": [],
      "source": [
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# display dataset information\n",
        "\n",
        "print(f'\\n\\n Sample of the train set of {len(X_train.index)} elements \\n')\n",
        "display(X_train.sample(n=5))\n",
        "\n",
        "print(f'\\n\\n Sample of the test set of {len(X_test.index)} elements \\n')\n",
        "display(X_test.sample(n=5))\n",
        "\n",
        "def plot_data(data, name):\n",
        "  labels = ['blues','classical', 'country', 'disco', 'hiphop', 'jazz', 'metal', 'pop', 'reggae', 'rock']\n",
        "  data = pd.Series(data)\n",
        "  counts = data.value_counts()\n",
        "  data = counts\n",
        "  colors = sns.color_palette('pastel')[0:10]\n",
        "  plt.pie(data, labels = labels, colors = colors, autopct='%.0f%%')\n",
        "  txt = f\"Classes distribution of the {name} set\"\n",
        "  plt.figtext(0.5, 0.01, txt, wrap=True, horizontalalignment='center', fontsize=14)\n",
        "  plt.savefig(f'my_plot_{name}.png')\n",
        "  print(\"\\n\\n\")\n",
        "  plt.show()\n",
        "\n",
        "plot_data(dataset_Y, \"dataset\")\n",
        "plot_data(y_train, \"train\")\n",
        "plot_data(y_test, \"test\")\n",
        "print(\"\\n\\n\")\n",
        "print(\"Dataset histogram\")\n",
        "display(pd.DataFrame(dataset_Y).plot(kind=\"hist\"))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GmF8EZaeDEUP"
      },
      "source": [
        "## Machine Learning Approaches"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UcrJPkDhD9XZ"
      },
      "source": [
        "### Naive Bayes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gANG2U70EFd8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5d90e7cb-9ea3-417b-f319-b48e77cccfe0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting the model...\n",
            "Model fitted. Prediting on validation set...\n",
            "Accuracy:  0.42311191992720654\n",
            "Precision:  0.43756242868268985\n",
            "Recall:  0.42209765212709993\n",
            "F1-micro:  0.42311191992720654\n",
            "F1-macro:  0.39816958714619793\n"
          ]
        }
      ],
      "source": [
        "from sklearn.naive_bayes import GaussianNB\n",
        "\n",
        "classifier = GaussianNB()\n",
        "accuracy, precision, recall, fmacro, fmicro = evaluate_classifier(classifier, 0, X_train, y_train, X_test, y_test, fit = True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sogi7x_lQcnF"
      },
      "source": [
        "### Nearest Neighbors"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w7wDqcY9QrCV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f22c71e6-1fa6-4954-c11f-b8a9b05d2bca"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting the model...\n",
            "Model fitted. Prediting on validation set...\n",
            "Accuracy:  0.2747849705749208\n",
            "Precision:  0.27326897510532777\n",
            "Recall:  0.27529945739057854\n",
            "F1-micro:  0.2747849705749208\n",
            "F1-macro:  0.27084436530461636\n"
          ]
        }
      ],
      "source": [
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.metrics import confusion_matrix, accuracy_score\n",
        "\n",
        "classifier = KNeighborsClassifier(n_neighbors=8)\n",
        "accuracy, precision, recall, fmacro, fmicro = evaluate_classifier(classifier, 0, X_train_split, y_train_split, X_validation, y_validation, fit = True) "
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from datetime import datetime\n",
        "\n",
        "def print_time():\n",
        "  now = datetime.now()\n",
        "  current_time = now.strftime(\"%H:%M:%S\")\n",
        "  print(\"Current Time =\", current_time)"
      ],
      "metadata": {
        "id": "Y5imv7U2ptJs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eRcKu_7-hfWN"
      },
      "source": [
        "### MULTI LAYER PERCEPRON"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UTR1BBWjhe9E",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "94997764-3cd3-4b72-f250-89da6091ada1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--------------\n",
            "\n",
            "Current Time = 20:30:55\n",
            "Fitting the model...\n",
            "Model fitted. Prediting on validation set...\n",
            "Accuracy:  0.10185604345857854\n",
            "Precision:  0.010190217391304348\n",
            "Recall:  0.1\n",
            "F1-micro:  0.10185604345857854\n",
            "F1-macro:  0.018495684340320593\n",
            "Current Time = 20:30:58\n",
            "\n",
            "--------------\n",
            "\n",
            "Current Time = 20:30:58\n",
            "Fitting the model...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model fitted. Prediting on validation set...\n",
            "Accuracy:  0.10185604345857854\n",
            "Precision:  0.010190217391304348\n",
            "Recall:  0.1\n",
            "F1-micro:  0.10185604345857854\n",
            "F1-macro:  0.018495684340320593\n",
            "Current Time = 20:31:01\n",
            "\n",
            "--------------\n",
            "\n",
            "Current Time = 20:31:01\n",
            "Fitting the model...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model fitted. Prediting on validation set...\n",
            "Accuracy:  0.10502489814395655\n",
            "Precision:  0.1768670309653916\n",
            "Recall:  0.103134991814364\n",
            "F1-micro:  0.10502489814395655\n",
            "F1-macro:  0.02550697181474707\n",
            "Current Time = 20:31:03\n",
            "\n",
            "--------------\n",
            "\n",
            "Current Time = 20:31:03\n",
            "Fitting the model...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model fitted. Prediting on validation set...\n",
            "Accuracy:  0.09597102761430512\n",
            "Precision:  0.009597102761430512\n",
            "Recall:  0.1\n",
            "F1-micro:  0.09597102761430512\n",
            "F1-macro:  0.01751342420487402\n",
            "Current Time = 20:31:07\n",
            "\n",
            "--------------\n",
            "\n",
            "Current Time = 20:31:07\n",
            "Fitting the model...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:641: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (250) reached and the optimization hasn't converged yet.\n",
            "  intercept_grads,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model fitted. Prediting on validation set...\n",
            "Accuracy:  0.10321412403802625\n",
            "Precision:  0.08522262607905498\n",
            "Recall:  0.10139534883720931\n",
            "F1-micro:  0.10321412403802625\n",
            "F1-macro:  0.021288777964742683\n",
            "Current Time = 20:31:11\n",
            "\n",
            "--------------\n",
            "\n",
            "Current Time = 20:31:11\n",
            "Fitting the model...\n",
            "Model fitted. Prediting on validation set...\n",
            "Accuracy:  0.10638297872340426\n",
            "Precision:  0.2602143182854537\n",
            "Recall:  0.10450899197118335\n",
            "F1-micro:  0.10638297872340424\n",
            "F1-macro:  0.028229036404984094\n",
            "Current Time = 20:31:24\n",
            "\n",
            "--------------\n",
            "\n",
            "Current Time = 20:31:24\n",
            "Fitting the model...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model fitted. Prediting on validation set...\n",
            "Accuracy:  0.11226799456767768\n",
            "Precision:  0.08091639172522237\n",
            "Recall:  0.11401144343550884\n",
            "F1-micro:  0.11226799456767768\n",
            "F1-macro:  0.06584622048274702\n",
            "Current Time = 20:31:29\n",
            "\n",
            "--------------\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        }
      ],
      "source": [
        "from sklearn.neural_network import MLPClassifier\n",
        "\n",
        "\n",
        "print(\"\\n--------------\\n\")\n",
        "\n",
        "print_time()\n",
        "classifier = MLPClassifier(hidden_layer_sizes=(128, 64, 8), random_state=1, max_iter=200, learning_rate_init=0.001)\n",
        "accuracy, precision, recall, fmacro, fmicro = evaluate_classifier(classifier, 0, X_train_split, y_train_split, X_validation, y_validation, fit = True) \n",
        "print_time()\n",
        "\n",
        "print(\"\\n--------------\\n\")\n",
        "\n",
        "print_time()\n",
        "classifier = MLPClassifier(hidden_layer_sizes=(128, 64, 8), random_state=1, max_iter=250, learning_rate_init=0.001)\n",
        "accuracy, precision, recall, fmacro, fmicro = evaluate_classifier(classifier, 0, X_train_split, y_train_split, X_validation, y_validation, fit = True) \n",
        "print_time()\n",
        "\n",
        "print(\"\\n--------------\\n\")\n",
        "\n",
        "print_time()\n",
        "classifier = MLPClassifier(hidden_layer_sizes=(128, 64, 8), random_state=1, max_iter=250, learning_rate_init=0.0001)\n",
        "accuracy, precision, recall, fmacro, fmicro = evaluate_classifier(classifier, 0, X_train_split, y_train_split, X_validation, y_validation, fit = True) \n",
        "print_time()\n",
        "\n",
        "print(\"\\n--------------\\n\")\n",
        "\n",
        "print_time()\n",
        "classifier = MLPClassifier(hidden_layer_sizes=(256, 128, 64, 8), random_state=1, max_iter=250, learning_rate_init=0.0001)\n",
        "accuracy, precision, recall, fmacro, fmicro = evaluate_classifier(classifier, 0, X_train_split, y_train_split, X_validation, y_validation, fit = True) \n",
        "print_time()\n",
        "\n",
        "print(\"\\n--------------\\n\")\n",
        "\n",
        "print_time()\n",
        "classifier = MLPClassifier(hidden_layer_sizes=(16, 8), random_state=1, max_iter=250, learning_rate_init=0.0001)\n",
        "accuracy, precision, recall, fmacro, fmicro = evaluate_classifier(classifier, 0, X_train_split, y_train_split, X_validation, y_validation, fit = True) \n",
        "print_time()\n",
        "\n",
        "print(\"\\n--------------\\n\")\n",
        "\n",
        "print_time()\n",
        "classifier = MLPClassifier(hidden_layer_sizes=(128, 64, 8), random_state=1, max_iter=250, learning_rate_init=0.00001)  #OPTIMAL\n",
        "accuracy, precision, recall, fmacro, fmicro = evaluate_classifier(classifier, 0, X_train_split, y_train_split, X_validation, y_validation, fit = True) \n",
        "print_time()\n",
        "\n",
        "print(\"\\n--------------\\n\")\n",
        "\n",
        "print_time()\n",
        "classifier = MLPClassifier(hidden_layer_sizes=(128, 64, 8), random_state=1, max_iter=250, learning_rate_init=0.000001)\n",
        "accuracy, precision, recall, fmacro, fmicro = evaluate_classifier(classifier, 0, X_train_split, y_train_split, X_validation, y_validation, fit = True) \n",
        "print_time()\n",
        "\n",
        "print(\"\\n--------------\\n\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lsqm4Tu1PJul"
      },
      "source": [
        "### SVC"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1tdnRWzzPJg0"
      },
      "outputs": [],
      "source": [
        "from sklearn.svm import LinearSVC\n",
        "\n",
        "classifier = LinearSVC(max_iter=10000)\n",
        "accuracy, precision, recall, fmacro, fmicro = evaluate_classifier(classifier, 0, X_train_split, y_train_split, X_validation, y_validation, fit = True) "
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### RANDOM FOREST"
      ],
      "metadata": {
        "id": "hQPgz7UaISV5"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t3nzcgD-f7p9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "673da9cc-f912-45af-fcb3-52464816b888"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Testing on test set\n",
            "Fitting the model...\n",
            "Model fitted. Prediting on validation set...\n",
            "Accuracy:  0.8229968311453146\n",
            "Precision:  0.8238986235818603\n",
            "Recall:  0.8233676916597078\n",
            "F1-micro:  0.8229968311453146\n",
            "F1-macro:  0.8215068861881493\n"
          ]
        }
      ],
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "import joblib as jb\n",
        "\n",
        "\"\"\"\n",
        "classifier_val = RandomForestClassifier(random_state=0)\n",
        "print(\"Fine-tuning on validation set\")\n",
        "accuracy, precision, recall, fmacro, fmicro = evaluate_classifier(classifier_val, 10, X_train_split=X_train, y_train_split=y_train, fit = True)\n",
        "print(\"\\n----------\\n\")\n",
        "\"\"\"\n",
        "\n",
        "classifier = RandomForestClassifier(random_state=0)\n",
        "print(\"Testing on test set\")\n",
        "accuracy, precision, recall, fmacro, fmicro = evaluate_classifier(classifier, 0, X_train_split, y_train_split, X_validation, y_validation, fit = True) \n",
        "# jb.dump(classifier, \"/content/gdrive/MyDrive/Music Classification/random_forest\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ADA BOOST"
      ],
      "metadata": {
        "id": "8cK6YQ5MIXZr"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ov0vYxDYg_pu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "45a25474-ab3a-4270-e1f5-ef785bbcc106"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting the model...\n",
            "Model fitted. Prediting on validation set...\n",
            "Accuracy:  0.4056133997283839\n",
            "Precision:  0.39288508667814837\n",
            "Recall:  0.4050988867392903\n",
            "F1-micro:  0.4056133997283839\n",
            "F1-macro:  0.36590077278004624\n"
          ]
        }
      ],
      "source": [
        "from sklearn.ensemble import AdaBoostClassifier\n",
        "\n",
        "classifier = AdaBoostClassifier(n_estimators=100, random_state=1)\n",
        "accuracy, precision, recall, fmacro, fmicro = evaluate_classifier(classifier, 0, X_train_split, y_train_split, X_validation, y_validation, fit = True)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### GRADIENT BOOSTING"
      ],
      "metadata": {
        "id": "eIIFGegTIa-n"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AD_pY73LpB4H",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7ebc6769-a8c3-4cca-c995-f6a2faa30b65"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting the model...\n",
            "Model fitted. Prediting on validation set...\n",
            "Accuracy:  0.8413709432817713\n",
            "Precision:  0.8421760134964179\n",
            "Recall:  0.8416580105337845\n",
            "F1-micro:  0.8413709432817713\n",
            "F1-macro:  0.8415255251505668\n"
          ]
        }
      ],
      "source": [
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "\n",
        "classifier = GradientBoostingClassifier(n_estimators=150, random_state=0)\n",
        "accuracy, precision, recall, fmacro, fmicro = evaluate_classifier(classifier, 0, X_train, y_train, X_test, y_test, fit = True)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### DECISION TREE"
      ],
      "metadata": {
        "id": "rR_tiHxUIiaX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.tree import DecisionTreeClassifier\n",
        "\n",
        "classifier = DecisionTreeClassifier(random_state=0)\n",
        "accuracy, precision, recall, fmacro, fmicro = evaluate_classifier(classifier, 0, X_train_split, y_train_split, X_validation, y_validation, fit = True)"
      ],
      "metadata": {
        "id": "aqYnM68eIhs0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "90452efa-0e47-4a09-f1f3-1fdaa9bf9728"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting the model...\n",
            "Model fitted. Prediting on validation set...\n",
            "Accuracy:  0.6011770031688547\n",
            "Precision:  0.5996293011596643\n",
            "Recall:  0.6015635048874498\n",
            "F1-micro:  0.6011770031688547\n",
            "F1-macro:  0.5995947197901336\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### LOGISTIC REGRESSION"
      ],
      "metadata": {
        "id": "MBWX1wwSNyu9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "#Failed to converge\n",
        "# MAX_ITER 100, 1000, 2000, 10000\n",
        "\n",
        "classifier = LogisticRegression(solver=\"sag\", random_state=0, max_iter=10000)\n",
        "accuracy, precision, recall, fmacro, fmicro = evaluate_classifier(classifier, 0, X_train, y_train, X_test, y_test, fit = True)"
      ],
      "metadata": {
        "id": "wJQd0_pZLPWx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bf3a36ae-f3ca-4abc-bdaf-973c056ddaa8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting the model...\n",
            "Model fitted. Prediting on validation set...\n",
            "Accuracy:  0.35426144980285107\n",
            "Precision:  0.32641808551651696\n",
            "Recall:  0.35419208065500796\n",
            "F1-micro:  0.3542614498028511\n",
            "F1-macro:  0.3190230063649076\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## DEEP LEARNING APPROCHES"
      ],
      "metadata": {
        "id": "GX-nTSaPF7sJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### FFNN"
      ],
      "metadata": {
        "id": "CwAoXpIzwd15"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import LabelBinarizer\n",
        "from sklearn.metrics import classification_report\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from keras.layers import Dense,Flatten,Input,Dropout,BatchNormalization\n",
        "from keras import models\n",
        "from sklearn.model_selection import train_test_split\n",
        "from keras import regularizers\n",
        "from keras.callbacks import LearningRateScheduler,EarlyStopping\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "reg = regularizers.l2(0.001)\n",
        "optimizer = tf.optimizers.Adam(learning_rate=0.01)\n",
        "\n",
        "# define the 784-256-128-10 architecture using Keras\n",
        "model = models.Sequential(name=\"BatchNorm\")\n",
        "\n",
        "model.add(Input(shape=(58,)))\n",
        "model.add(BatchNormalization())\n",
        "\n",
        "#Dense                    50ep      100ep     200ep\n",
        "#128 64 32\n",
        "#128 64 32 10\n",
        "#64 32 16\n",
        "#64 32 32 16\n",
        "#64 32 32     \n",
        "#32 16 16                 .77\n",
        "#32 16                    .78      .81        0.79           OPTIMAL\n",
        "#32 16 10                 .76      .80\n",
        "\n",
        "#model.add(Dense(64,activation='relu',kernel_regularizer=reg))\n",
        "# model.add(Dense(64,activation='relu',kernel_regularizer=reg))\n",
        "model.add(Dense(32,activation='relu',kernel_regularizer=reg))\n",
        "model.add(Dense(16,activation='relu',kernel_regularizer=reg))\n",
        "#model.add(Dense(10,activation='relu',kernel_regularizer=reg))\n",
        "\n",
        "\n",
        "# model.add(Dropout(0.5)) \n",
        "\n",
        "\n",
        "model.add(Dense(10,activation='softmax',name='output'))\n",
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5aL11YIWwdEc",
        "outputId": "84e32a8b-b7cb-4f83-92d0-27650ac2f6e5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"BatchNorm\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " batch_normalization_16 (Bat  (None, 58)               232       \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " dense_39 (Dense)            (None, 32)                1888      \n",
            "                                                                 \n",
            " dense_40 (Dense)            (None, 16)                528       \n",
            "                                                                 \n",
            " output (Dense)              (None, 10)                170       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 2,818\n",
            "Trainable params: 2,702\n",
            "Non-trainable params: 116\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "\n",
        "# train the model using SGD\n",
        "print(\"[INFO] training network...\")\n",
        "\n",
        "model.compile(optimizer=optimizer,loss='sparse_categorical_crossentropy',metrics=['accuracy'])\n",
        "\n",
        "# X_train_split_ffnn = np.asarray(X_train).astype('float32')\n",
        "# X_validation_ffnn = np.asarray(X_validation).astype('float32')\n",
        "\n",
        "H = model.fit(X_train, y_train, validation_split=0.33, epochs=200, batch_size=32)"
      ],
      "metadata": {
        "id": "5SV0n2six3BT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "H = model.fit(X_train, y_train, epochs=100, batch_size=32)"
      ],
      "metadata": {
        "id": "mX7VZxTG828n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "X_test_ffnn =  np.asarray(X_test).astype('float32')\n",
        "model.evaluate(X_test, y_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "N2gNdV635dkk",
        "outputId": "fc910396-ac29-4f16-c3a1-74c7c659e03c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "104/104 [==============================] - 1s 8ms/step - loss: 0.7439 - accuracy: 0.8062\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\naccuracy, precision, recall, fmacro, fmicro = calculate_metrics(predicted, y_test)\\n   \\nprint(\"Accuracy: \", accuracy)\\nprint(\"Precision: \", precision)\\nprint(\"Recall: \", recall)\\nprint(\"F1-micro: \", fmicro)\\nprint(\"F1-macro: \", fmacro)'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 71
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "```\n",
        "\n",
        "\n",
        "\n",
        "```\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "```\n",
        "\n",
        "\n",
        "\n",
        "```"
      ],
      "metadata": {
        "id": "gVBYzsQckZTz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# MFCC IMAGE CLASSIFICAITON"
      ],
      "metadata": {
        "id": "nb4E1HD4QeW1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "print('tf.test.is_gpu_available:', tf.test.is_gpu_available())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bYXljiz0_Y-B",
        "outputId": "a25128b7-b077-4d7d-92f6-459c7072c281"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tf.test.is_gpu_available: True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Dataset Preprocessing"
      ],
      "metadata": {
        "id": "J0AT0q7KQnVE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/gdrive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ywuCI_sEfW1Q",
        "outputId": "ad13d5c0-d23b-4b74-d050-526b5374c11a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import os\n",
        "import math\n",
        "import librosa\n",
        "import joblib\n",
        "\n",
        "DATASET_PATH = \"/content/dataset/genres_original\"\n",
        "JSON_PATH = \"./data_10.json\"\n",
        "SAMPLE_RATE = 22050\n",
        "TRACK_DURATION = 30 # measured in seconds\n",
        "SAMPLES_PER_TRACK = SAMPLE_RATE * TRACK_DURATION\n",
        "not_allowed = \"/content/dataset/genres_original/jazz/jazz.00054.wav\"\n",
        "\n",
        "def save_mfcc(dataset_path, json_path, num_mfcc=13, n_fft=2048, hop_length=512, num_segments=5):\n",
        "    \"\"\"Extracts MFCCs from music dataset and saves them into a json file along witgh genre labels.\n",
        "\n",
        "        :param dataset_path (str): Path to dataset\n",
        "        :param json_path (str): Path to json file used to save MFCCs\n",
        "        :param num_mfcc (int): Number of coefficients to extract\n",
        "        :param n_fft (int): Interval we consider to apply FFT. Measured in # of samples\n",
        "        :param hop_length (int): Sliding window for FFT. Measured in # of samples\n",
        "        :param: num_segments (int): Number of segments we want to divide sample tracks into\n",
        "        :return:\n",
        "        \"\"\"\n",
        "\n",
        "    # dictionary to store mapping, labels, and MFCCs\n",
        "    data = {\n",
        "        \"mapping\": [],\n",
        "        \"labels\": [],\n",
        "        \"mfcc\": []\n",
        "    }\n",
        "    \n",
        "    samples_per_segment = int(SAMPLES_PER_TRACK / num_segments)\n",
        "    num_mfcc_vectors_per_segment = math.ceil(samples_per_segment / hop_length)\n",
        "\n",
        "    # loop through all genre sub-folder\n",
        "    for i, (dirpath, dirnames, filenames) in enumerate(os.walk(dataset_path)):\n",
        "\n",
        "        # ensure we're processing a genre sub-folder level\n",
        "        if dirpath is not dataset_path:\n",
        "\n",
        "            # save genre label (i.e., sub-folder name) in the mapping\n",
        "            semantic_label = dirpath.split(\"/\")[-1]\n",
        "            data[\"mapping\"].append(semantic_label)\n",
        "            print(\"\\nProcessing: {}\".format(semantic_label))\n",
        "\n",
        "            # process all audio files in genre sub-dir\n",
        "            for f in filenames:\n",
        "               \n",
        "\t\t# load audio file\n",
        "                file_path = os.path.join(dirpath, f)\n",
        "                if file_path != not_allowed :\n",
        "                    signal, sample_rate = librosa.load(file_path, sr=SAMPLE_RATE)\n",
        "\n",
        "                # process all segments of audio file\n",
        "                    for d in range(num_segments):\n",
        "\n",
        "                    # calculate start and finish sample for current segment\n",
        "                        start = samples_per_segment * d\n",
        "                        finish = start + samples_per_segment\n",
        "\n",
        "                    # extract mfcc\n",
        "                        mfcc = librosa.feature.mfcc(signal[start:finish], sample_rate, n_mfcc=num_mfcc, n_fft=n_fft, hop_length=hop_length)\n",
        "                        mfcc = mfcc.T\n",
        "\n",
        "                    # store only mfcc feature with expected number of vectors\n",
        "                        if len(mfcc) == num_mfcc_vectors_per_segment:\n",
        "                            data[\"mfcc\"].append(mfcc.tolist())\n",
        "                            data[\"labels\"].append(i-1)\n",
        "                            print(\"{}, segment:{}\".format(file_path, d+1))\n",
        "\n",
        "    # save MFCCs to json file\n",
        "    with open(json_path, \"w\") as fp:\n",
        "        json.dump(data, fp, indent=4) \n",
        "                \n",
        "save_mfcc(DATASET_PATH, JSON_PATH, num_segments=10)\n"
      ],
      "metadata": {
        "id": "_fJmhdmu8byM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import numpy as np\n",
        "DATASET_PATH = \"/content/gdrive/MyDrive/Music Classification/data_10.json\"\n",
        "def load_data(dataset_path):\n",
        "    with open(dataset_path,\"r\") as fp:\n",
        "        data = json.load(fp)\n",
        "    inputs = np.array(data[\"mfcc\"])  \n",
        "    targets = np.array(data[\"labels\"])   \n",
        "    \n",
        "    return inputs , targets\n",
        "\n",
        "inputs,targets = load_data(DATASET_PATH)"
      ],
      "metadata": {
        "id": "-hyXZAUt__Pm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(inputs, targets,test_size=0.33, random_state = 0)\n",
        "X_train_split, X_validation, y_train_split, y_validation = train_test_split(X_train, y_train, test_size=0.3, random_state = 0)"
      ],
      "metadata": {
        "id": "5T8z6V4RABpW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"X_train: {X_train.shape}\")\n",
        "print(f\"y_train: {y_train.shape}\")\n",
        "print(\"\\n\")\n",
        "print(f\"X_test: {X_test.shape}\")\n",
        "print(f\"y_test: {y_test.shape}\")\n",
        "print(\"\\n\")\n",
        "print(f\"X_train_split: {X_train_split.shape}\")\n",
        "print(f\"y_train_split: {y_train_split.shape}\")\n",
        "print(\"\\n\")\n",
        "print(f\"X_validation: {X_validation.shape}\")\n",
        "print(f\"y_validation: {y_validation.shape}\")"
      ],
      "metadata": {
        "id": "eyXXHqU-N-6P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## (Deep) Machine Learning Approches"
      ],
      "metadata": {
        "id": "s2g-WBKYQynC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### FFNN"
      ],
      "metadata": {
        "id": "RuAA4xKNyBU8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow import keras\n",
        "from tensorflow.keras.layers import Flatten, Dense, Input, Dropout\n",
        "\n",
        "def create_model_FFNN():\n",
        "  model = keras.Sequential([\n",
        "      Input(shape=(130,13,)),\n",
        "      Flatten(),\n",
        "      # Dense(512,activation=\"relu\"),\n",
        "      # Dense(256,activation=\"relu\"),\n",
        "      Dense(128,activation=\"relu\"),\n",
        "      Dropout(0.4),\n",
        "      Dense(64,activation=\"relu\"),\n",
        "      Dense(32, activation=\"relu\"),\n",
        "      Dense(16, activation=\"relu\"),\n",
        "      Dense(10,activation=\"softmax\"),\n",
        "  ])\n",
        "\n",
        "  #64 32 16 10        .42\n",
        "  #128 64 32 16 10        .42\n",
        "\n",
        "  optimizer = keras.optimizers.Adam(learning_rate = 0.0001)\n",
        "  model.compile(optimizer=optimizer,loss=\"sparse_categorical_crossentropy\",metrics=[\"accuracy\"])\n",
        "  model.summary()\n",
        "  return model"
      ],
      "metadata": {
        "id": "p4XPOthdAqla"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_tuning = create_model_FFNN()\n",
        "history_fnn = model_tuning.fit(X_train_split, y_train_split, validation_data=(X_validation, y_validation), epochs=50, batch_size=32)"
      ],
      "metadata": {
        "id": "GG68ki8LBBdw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "final_model = create_model_FFNN()\n",
        "history_final = final_model.fit(X_train, y_train, epochs=50, batch_size=32)"
      ],
      "metadata": {
        "id": "jXywXi43TUFn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "final_model.evaluate(X_test, y_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-s9X2rGeYM25",
        "outputId": "cc97d236-94b8-40a3-c94a-a5b280edb63b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "94/94 [==============================] - 0s 3ms/step - loss: 1.7207 - accuracy: 0.3862\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[1.720725417137146, 0.3861815631389618]"
            ]
          },
          "metadata": {},
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### CNN"
      ],
      "metadata": {
        "id": "8rDwnVNgyLOA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow import keras\n",
        "from tensorflow.keras.layers import Flatten, Dense, Input, Dropout, Conv2D, MaxPooling2D, BatchNormalization\n",
        "from tensorflow.keras import regularizers\n",
        "\n",
        "def build_model():\n",
        "    model = keras.Sequential([\n",
        "        Input(shape=((130, 13, 1,))),\n",
        "        BatchNormalization(),\n",
        "\n",
        "        Conv2D(8 , kernel_size=(3,3), activation = 'relu', kernel_regularizer=regularizers.l2(l=0.01), kernel_initializer='he_normal'),\n",
        "        MaxPooling2D((2,2),strides=(2,2),padding='same'),\n",
        "        \n",
        "        #Dropout(0.3),\n",
        "        \n",
        "        Conv2D(32 , kernel_size=(3,3), activation = 'relu', kernel_regularizer=regularizers.l2(l=0.01), kernel_initializer='he_normal'),\n",
        "        MaxPooling2D((2,2),strides=(2,2),padding='same'),\n",
        "        #BatchNormalization(),\n",
        "        # Dropout(0.3),\n",
        "        #BatchNormalization(),   \n",
        "\n",
        "        #Conv2D(16 , kernel_size=(3,3), activation = 'relu', kernel_regularizer=regularizers.l2(l=0.01), kernel_initializer='he_normal'),\n",
        "        #MaxPooling2D((2,2),strides=(2,2),padding='same'),\n",
        "        #BatchNormalization(),\n",
        "\n",
        "        Flatten(),\n",
        "        Dense(128, activation = 'relu'),\n",
        "        Dense(64, activation = 'relu'),\n",
        "        # Dense(32, activation = 'relu'),\n",
        "        #Dropout(0.3),\n",
        "        Dense(10, activation='softmax')\n",
        "\n",
        "\n",
        "    ])\n",
        "\n",
        "    #model.add(keras.layers.Conv2D(64 , (3,3) ,activation = 'relu'))\n",
        "    #model.add(keras.layers.MaxPooling2D((3,3),strides=(2,2),padding='same'))\n",
        "    #model.add(keras.layers.BatchNormalization())\n",
        "    \n",
        "    optimizer_cnn =  keras.optimizers.Adam(learning_rate=0.0001)\n",
        "    model.compile(optimizer = optimizer_cnn, loss = 'sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "    model.summary()\n",
        "    \n",
        "    return model"
      ],
      "metadata": {
        "id": "y_F_JtK3AVi-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "DATASET_PATH = \"/content/gdrive/MyDrive/Music Classification/data_10.json\"\n",
        "\n",
        "def prepare_datasets(X_train, X_test):\n",
        "    \n",
        "    X_train = X_train[..., np.newaxis] # (num_samples,130,13,1)\n",
        "    X_test = X_test[..., np.newaxis]\n",
        "    \n",
        "    return X_train, X_test"
      ],
      "metadata": {
        "id": "O1R7Ie2rAqNB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train_cnn, X_test_cnn = prepare_datasets(X_train, X_test)\n",
        "X_train_split_cnn, X_validation_cnn = prepare_datasets(X_train_split, X_validation)"
      ],
      "metadata": {
        "id": "UxiEtJlJAsOk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import joblib as jb\n",
        "\n",
        "jb.dump(X_test_cnn, \"/content/gdrive/MyDrive/Music Classification/X_test_cnn\")\n",
        "jb.dump(y_test, \"/content/gdrive/MyDrive/Music Classification/y_test_cnn\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IHtAtZAWE1Vi",
        "outputId": "67ff77dc-9b8b-4b5d-93e5-d43e8198712f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['/content/gdrive/MyDrive/Music Classification/y_test_cnn']"
            ]
          },
          "metadata": {},
          "execution_count": 64
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"X_train: {X_train_cnn.shape}\")\n",
        "print(f\"y_train: {y_train.shape}\")\n",
        "print(\"\\n\")\n",
        "print(f\"X_test: {X_test_cnn.shape}\")\n",
        "print(f\"y_test: {y_test.shape}\")\n",
        "print(\"\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F9NtRgPUB1Fm",
        "outputId": "a17383f3-9337-496f-fc97-e296ae1e2582"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "X_train: (6690, 130, 13, 1)\n",
            "y_train: (6690,)\n",
            "\n",
            "\n",
            "X_test: (3296, 130, 13, 1)\n",
            "y_test: (3296,)\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_cnn = build_model()\n",
        "\n",
        "model_cnn.fit(X_train_split_cnn, \n",
        "              y_train_split, \n",
        "              validation_data=(X_validation_cnn, y_validation), \n",
        "              batch_size=32,\n",
        "              epochs=50)"
      ],
      "metadata": {
        "id": "ixG-TadWAi3C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_cnn = build_model()\n",
        "\n",
        "model_cnn.fit(X_train_cnn, \n",
        "              y_train, \n",
        "              batch_size=32,\n",
        "              epochs=50)"
      ],
      "metadata": {
        "id": "GolprthpU0Fq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_cnn.evaluate(X_test_cnn, y_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DsKY2AQAVoih",
        "outputId": "4429d1d1-d040-4db5-e81d-d5ef85e5ed69"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "94/94 [==============================] - 1s 4ms/step - loss: 1.2605 - accuracy: 0.6702\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[1.2605401277542114, 0.6702269911766052]"
            ]
          },
          "metadata": {},
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import joblib as jb\n",
        "\n",
        "jb.dump(model_cnn, \"/content/gdrive/MyDrive/Music Classification/model_cnn\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MDWLAH0qc_B0",
        "outputId": "293fd92c-65b8-4b0d-f8be-1ec75574bab9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['/content/gdrive/MyDrive/Music Classification/model_cnn']"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### RNN"
      ],
      "metadata": {
        "id": "QhU2KhkfykmX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_train_rnn =  jb.load(\"/content/gdrive/MyDrive/Music Classification/rnn_X_train\")\n",
        "y_train_rnn = jb.load(\"/content/gdrive/MyDrive/Music Classification/rnn_y_train\")\n",
        "X_test_rnn = jb.load(\"/content/gdrive/MyDrive/Music Classification/rnn_X_test\")\n",
        "y_test_rnn = jb.load(\"/content/gdrive/MyDrive/Music Classification/rnn_y_test\")\n",
        "\n",
        "X_train_split, X_validation, y_train_split, y_validation = train_test_split(X_train_rnn, y_train_rnn, test_size=0.33, random_state=0)"
      ],
      "metadata": {
        "id": "FoxIw4Lty7hO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow import keras\n",
        "\n",
        "def build_model():\n",
        "    model = keras.Sequential()\n",
        "    model.add(keras.layers.Input(shape=(130, 13,)))\n",
        "    model.add(keras.layers.LSTM(64,return_sequences=True))\n",
        "    model.add(keras.layers.LSTM(64))\n",
        "    model.add(keras.layers.Dense(64,activation='relu'))\n",
        "    model.add(keras.layers.Dropout(0.3))\n",
        "    model.add(keras.layers.Dense(10,activation='softmax'))\n",
        "    model.summary()\n",
        "\n",
        "    return model"
      ],
      "metadata": {
        "id": "wAyMlBJrTvog"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_rnn = build_model()\n",
        "optimizer =  keras.optimizers.Adam(learning_rate=0.001)\n",
        "model_rnn.compile(optimizer = optimizer ,loss = 'sparse_categorical_crossentropy',metrics=['accuracy'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DnxUyaVufVku",
        "outputId": "c64b78c0-5319-4b04-b1a9-3c8caa3d4faf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_10\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " lstm_4 (LSTM)               (None, 130, 64)           19968     \n",
            "                                                                 \n",
            " lstm_5 (LSTM)               (None, 64)                33024     \n",
            "                                                                 \n",
            " dense_28 (Dense)            (None, 64)                4160      \n",
            "                                                                 \n",
            " dropout_2 (Dropout)         (None, 64)                0         \n",
            "                                                                 \n",
            " dense_29 (Dense)            (None, 10)                650       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 57,802\n",
            "Trainable params: 57,802\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_rnn.fit(X_train_rnn, y_train_rnn, validation_data = (X_validation, y_validation), batch_size=32, epochs=30)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vSaHx358fjj8",
        "outputId": "1a722b98-cbbe-41f5-9eb2-78d8d6a74833"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/30\n",
            "210/210 [==============================] - 8s 39ms/step - loss: 1.4575 - accuracy: 0.4726 - val_loss: 1.3195 - val_accuracy: 0.5274\n",
            "Epoch 2/30\n",
            "210/210 [==============================] - 6s 27ms/step - loss: 1.3330 - accuracy: 0.5198 - val_loss: 1.2239 - val_accuracy: 0.5650\n",
            "Epoch 3/30\n",
            "210/210 [==============================] - 6s 27ms/step - loss: 1.2299 - accuracy: 0.5676 - val_loss: 1.1509 - val_accuracy: 0.5835\n",
            "Epoch 4/30\n",
            "210/210 [==============================] - 5s 22ms/step - loss: 1.2033 - accuracy: 0.5748 - val_loss: 1.0724 - val_accuracy: 0.6139\n",
            "Epoch 5/30\n",
            "210/210 [==============================] - 5s 22ms/step - loss: 1.1482 - accuracy: 0.5969 - val_loss: 1.1148 - val_accuracy: 0.6030\n",
            "Epoch 6/30\n",
            "210/210 [==============================] - 5s 22ms/step - loss: 1.0895 - accuracy: 0.6184 - val_loss: 0.9628 - val_accuracy: 0.6627\n",
            "Epoch 7/30\n",
            "210/210 [==============================] - 5s 23ms/step - loss: 1.0023 - accuracy: 0.6546 - val_loss: 0.8840 - val_accuracy: 0.7048\n",
            "Epoch 8/30\n",
            "210/210 [==============================] - 5s 23ms/step - loss: 0.9319 - accuracy: 0.6803 - val_loss: 0.8156 - val_accuracy: 0.7180\n",
            "Epoch 9/30\n",
            "210/210 [==============================] - 5s 24ms/step - loss: 0.8845 - accuracy: 0.6936 - val_loss: 0.8109 - val_accuracy: 0.7121\n",
            "Epoch 10/30\n",
            "210/210 [==============================] - 5s 24ms/step - loss: 0.8354 - accuracy: 0.7147 - val_loss: 0.7835 - val_accuracy: 0.7302\n",
            "Epoch 11/30\n",
            "210/210 [==============================] - 5s 23ms/step - loss: 0.8136 - accuracy: 0.7178 - val_loss: 0.7506 - val_accuracy: 0.7311\n",
            "Epoch 12/30\n",
            "210/210 [==============================] - 5s 24ms/step - loss: 0.8008 - accuracy: 0.7284 - val_loss: 0.6772 - val_accuracy: 0.7614\n",
            "Epoch 13/30\n",
            "210/210 [==============================] - 6s 28ms/step - loss: 0.7372 - accuracy: 0.7522 - val_loss: 0.6182 - val_accuracy: 0.7963\n",
            "Epoch 14/30\n",
            "210/210 [==============================] - 5s 24ms/step - loss: 0.6757 - accuracy: 0.7722 - val_loss: 0.5789 - val_accuracy: 0.8035\n",
            "Epoch 15/30\n",
            "210/210 [==============================] - 5s 22ms/step - loss: 0.6494 - accuracy: 0.7867 - val_loss: 0.5344 - val_accuracy: 0.8203\n",
            "Epoch 16/30\n",
            "210/210 [==============================] - 4s 19ms/step - loss: 0.6178 - accuracy: 0.7926 - val_loss: 0.6751 - val_accuracy: 0.7823\n",
            "Epoch 17/30\n",
            "210/210 [==============================] - 6s 29ms/step - loss: 0.6107 - accuracy: 0.7967 - val_loss: 0.4894 - val_accuracy: 0.8357\n",
            "Epoch 18/30\n",
            "210/210 [==============================] - 7s 33ms/step - loss: 0.5334 - accuracy: 0.8219 - val_loss: 0.4733 - val_accuracy: 0.8416\n",
            "Epoch 19/30\n",
            "210/210 [==============================] - 4s 20ms/step - loss: 0.5562 - accuracy: 0.8147 - val_loss: 0.6070 - val_accuracy: 0.8044\n",
            "Epoch 20/30\n",
            "210/210 [==============================] - 5s 23ms/step - loss: 0.5268 - accuracy: 0.8268 - val_loss: 0.4792 - val_accuracy: 0.8325\n",
            "Epoch 21/30\n",
            "210/210 [==============================] - 5s 24ms/step - loss: 0.4841 - accuracy: 0.8399 - val_loss: 0.4228 - val_accuracy: 0.8551\n",
            "Epoch 22/30\n",
            "210/210 [==============================] - 5s 25ms/step - loss: 0.4842 - accuracy: 0.8369 - val_loss: 0.4543 - val_accuracy: 0.8420\n",
            "Epoch 23/30\n",
            "210/210 [==============================] - 6s 29ms/step - loss: 0.4500 - accuracy: 0.8517 - val_loss: 0.3593 - val_accuracy: 0.8796\n",
            "Epoch 24/30\n",
            "210/210 [==============================] - 5s 26ms/step - loss: 0.4202 - accuracy: 0.8600 - val_loss: 0.3341 - val_accuracy: 0.8837\n",
            "Epoch 25/30\n",
            "210/210 [==============================] - 5s 22ms/step - loss: 0.4141 - accuracy: 0.8615 - val_loss: 0.4070 - val_accuracy: 0.8642\n",
            "Epoch 26/30\n",
            "210/210 [==============================] - 5s 25ms/step - loss: 0.3837 - accuracy: 0.8710 - val_loss: 0.5195 - val_accuracy: 0.8339\n",
            "Epoch 27/30\n",
            "210/210 [==============================] - 4s 19ms/step - loss: 0.4265 - accuracy: 0.8564 - val_loss: 0.3711 - val_accuracy: 0.8737\n",
            "Epoch 28/30\n",
            "210/210 [==============================] - 5s 22ms/step - loss: 0.3798 - accuracy: 0.8755 - val_loss: 0.3085 - val_accuracy: 0.9031\n",
            "Epoch 29/30\n",
            "210/210 [==============================] - 4s 19ms/step - loss: 0.3169 - accuracy: 0.8930 - val_loss: 0.2618 - val_accuracy: 0.9172\n",
            "Epoch 30/30\n",
            "210/210 [==============================] - 5s 24ms/step - loss: 0.3127 - accuracy: 0.8973 - val_loss: 0.2655 - val_accuracy: 0.9072\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fce420874d0>"
            ]
          },
          "metadata": {},
          "execution_count": 56
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_rnn.evaluate(X_test_rnn, y_test_rnn)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_XZXVeWElTBx",
        "outputId": "d595de35-e985-4299-cac1-8ed77ec88412"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "103/103 [==============================] - 3s 13ms/step - loss: 0.7356 - accuracy: 0.7815\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.7355664968490601, 0.7814871072769165]"
            ]
          },
          "metadata": {},
          "execution_count": 76
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ENSEMBLE LEARNING\n",
        "CNN + RANDOM FOREST"
      ],
      "metadata": {
        "id": "Nu2WGG5Ede2t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/gdrive')"
      ],
      "metadata": {
        "id": "q-vlxD6MEvN1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1c8810d3-5939-458e-f35f-062193e39922"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import joblib as jb\n",
        "\n",
        "X_test = jb.load(\"/content/gdrive/MyDrive/Music Classification/X_test\")\n",
        "y_test = jb.load(\"/content/gdrive/MyDrive/Music Classification/y_test\")\n",
        "\n",
        "file_name_test = [filename for filename in X_test[\"filename\"]]\n",
        "index_test = [dataset.index[dataset[\"filename\"] == file_name][0] for file_name in file_name_test]\n",
        "index_test = [index for index in index_test if index <= 9986]\n",
        "\n",
        "targets_cnn = jb.load(\"/content/gdrive/MyDrive/Music Classification/targets_cnn\")\n",
        "inputs_cnn = jb.load(\"/content/gdrive/MyDrive/Music Classification/inputs_cnn\")\n",
        "\n",
        "# model_cnn_dump = jb.load(\"/content/gdrive/MyDrive/Music Classification/model_cnn\")\n",
        "random_forest_dump = jb.load(\"/content/gdrive/MyDrive/Music Classification/random_forest\")\n",
        "\n",
        "# jb.load(\"/content/gdrive/MyDrive/Music Classification/rnn_X_train\")\n",
        "# jb.load(\"/content/gdrive/MyDrive/Music Classification/rnn_y_train\")\n",
        "X_test_rnn = jb.load(\"/content/gdrive/MyDrive/Music Classification/rnn_X_test\")\n",
        "y_test_rnn = jb.load(\"/content/gdrive/MyDrive/Music Classification/rnn_y_test\")\n",
        "model_rnn_dump = jb.load(\"/content/gdrive/MyDrive/Music Classification/rnn_model\")\n",
        "\n"
      ],
      "metadata": {
        "id": "TiRONnLVdeFx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"X_test: {X_test.shape}\")\n",
        "print(f\"y_test: {y_test.shape}\")\n",
        "print(f\"Dataset cnn: {inputs_cnn.shape}\")\n",
        "print(f\"Target cnn: {targets_cnn.shape}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "igivZ89tHWn3",
        "outputId": "d0392f46-5057-4a2f-c2fc-9b246f03bf59"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "X_test: (3297, 59)\n",
            "y_test: (3297,)\n",
            "Dataset cnn: (9986, 130, 13, 1)\n",
            "Target cnn: (9986, 1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import warnings\n",
        "\n",
        "def ensamble_classifier_predicition(dataset, inputs_cnn, targets_cnn, model_dl, model_rf):\n",
        "  counter = 1\n",
        "  rigth_prediction = 0\n",
        "\n",
        "  for i in index_test:\n",
        "    X_current = dataset.iloc[i].drop([\"filename\", \"label\"], axis=0)\n",
        "    y_current = dataset.iloc[i][\"label\"]\n",
        "    X_cnn_current = inputs_cnn[i]\n",
        "    y_cnn_current = targets_cnn[i][0]\n",
        "\n",
        "    #print(f\"Expected value {y_current} \\n\\n\")\n",
        "    print(f\">> {counter}/{len(index_test)}\")\n",
        "    \n",
        "    input = np.array(X_current).reshape(1, -1)\n",
        "\n",
        "    probs_kf = _evalute_model_rf(model_rf, input)\n",
        "\n",
        "    input =  np.array([X_cnn_current,])\n",
        "    probs_dl = _evalute_model_dl(model_dl, input)\n",
        "\n",
        "    mean_prob = np.mean( np.array([ probs_kf, probs_dl ]), axis=0 )\n",
        "    pred = get_prediction(mean_prob)\n",
        "\n",
        "    counter += 1\n",
        "\n",
        "    \n",
        "    if(pred == y_current):\n",
        "      rigth_prediction += 1\n",
        "    \n",
        "  return rigth_prediction\n",
        "\n",
        "\n",
        "\n",
        "def _evalute_model_rf(model, input):\n",
        "  \n",
        "  probs = random_forest_dump.predict_proba(input)\n",
        "  #print(\"RandomForet\")\n",
        "  #_print_probs(probs)\n",
        "  return probs\n",
        "\n",
        "\n",
        "def _evalute_model_dl(model, input):\n",
        "  probs = model.predict(input)\n",
        "  #print(\"CNN\")\n",
        "  #_print_probs(probs)\n",
        "  return probs\n",
        "\n",
        "\n",
        "def get_prediction(mean_prob):\n",
        "  #for prob in mean_prob[0]:\n",
        "  #  print(f'{prob:.20f}\\n\\n')\n",
        "\n",
        "  hig_prob = np.amax(mean_prob)\n",
        "  #print(f\"Highest probability: {hig_prob}\")\n",
        "  max_index = np.where(mean_prob[0] == hig_prob)\n",
        "  #print(f\"\\n\\nMax index: {max_index[0][0]}\")\n",
        "  return max_index[0][0]\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "warnings.filterwarnings('ignore')\n",
        "right_prediction = ensamble_classifier_predicition(dataset, inputs_cnn, targets_cnn, model_rnn_dump, random_forest_dump)\n",
        "print(\"\\n\\n--------------------------------------\\n\\n\")\n",
        "print(right_prediction)\n",
        "print(f\"Accuracy: {right_prediction/len(index_test)}\")"
      ],
      "metadata": {
        "id": "9leB9yzdXbJu"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "MCJKmgpJiVj2",
        "GmF8EZaeDEUP",
        "sogi7x_lQcnF",
        "rR_tiHxUIiaX",
        "GX-nTSaPF7sJ",
        "nb4E1HD4QeW1",
        "J0AT0q7KQnVE"
      ],
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}